gameplay elements: 
    1. worth fiddling arround with self.attack_cooldown and self.parry_cooldown (and self.action_cooldown) in fighter.py; 
    a totally random AI can spam attacks very quickly making it nontrivial to defeat it!
    2. maybe change the parry rect width to make parrying a little less difficult? right now it's set at 2 * self.rect.width which is pretty okay
    3. maybe increase attack, parry, action cooldown if gameplay agent is AI?

hyperparameters as not-function-parameter locations: 
    - round win/loss/tie reward in 'game.py''s step() function
    * REMOVED AS OF 02/23/25:
        - concurrent location reward penalty in same location as ^above^ (to encourage combat)
        - fighter ideal distance reward shaping to encourage combat (same location as ^above^)
        - constant penalty reward shaping (self.health - target.health - 100) to encourage combat (same location as ^above^)
        
other important hyperparameters: 
    - game_max_frames:
        - if game exceeds this number of frames, it's like a tie - add. penalty for not winning in fixed time period. game resets.

to-do's: 
    1. ✅ in fighter.py: 
        1.1. ✅ write get_state function 
        1.2. ✅ write parse_keys_from_action_idx function

    from 02/23/23: 
    1. ✅ solve environment (need pygame and pytorch)
    2. ✅ check to make sure demo.py works as intended
        2.1. ✅ in playtest, up action cooldown for AI agents if too fast. 
               - 02/24 note: AI_RANDOM/AI_TRAINER cooldown for something = HUMAN cooldown * 3
    3. ✅ write stable_softmax & epsilon greedy functions in trainer.py 
        - note from 02/25: currently, epsilon greedy is used. 
    4. ✅ fin. trainer.py's train function! 
        4.1. ✅ write train_step function in trainer.py

    long-term: 
    0. ✅ prelim. train model! 
        0.1. ✅ do prelim. train with few training episodes, just to make sure everything is working. 
        0.2. ✅ if prelim. train works, update train() to write loss & avg. reward to files for viz. 

    1. ✅ train model verbose! 
        1.1. ✅ make sure loss + reward writing funcs. work before letting trainer run on big train.

    2. after model has been trained: 
        2.1. re-visit demo & make sure loading loaded-params AI_TRAINER agent works!
        2.2. run AI_TRAINER vs AI_RAND test!
        2.3. run AI_TRAINER vs HUMAN   test!