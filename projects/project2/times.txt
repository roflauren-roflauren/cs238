small.csv:
    - train (q-learning): 0.328 seconds 
    - total: 0.343 seconds

    - optimal params/notes:
        - had a min. state val. - if Q(s,a*) < min_state_val, 
         selected action that moved toward max value state 
         instead of a* = max_a Q(s,a)
         - default st val: 0
         - eta: 0.05

medium.csv:
    - train (q-learning): 26.826
    - total: 27.058

    - used params/notes:
        - if state value after q-learning is still default, just picked random action 
          for policy.
        - reward shaping: subtract distance from goal (assumed to be right-most position) 
                          from reward to encourage progression toward goal. 
        - default st val: -500
        - eta: 0.05
        - train loops: 3

large.csv:
    - train: 7958 seconds 
    - policy extraction: 139 
    - overall: 8097 

    - used params/notes: 
      - two hidden layers, 12 nodes in each 
      - 20 replays 
      - lr: 6e-4
      - copy weights to target from training nn every 1000 episodes