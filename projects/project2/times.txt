small.csv:
    - train (q-learning): 0.328 seconds 
    - total: 0.343 seconds

    - optimal params/notes:
        - had a min. state val. - if Q(s,a*) < min_state_val, 
         selected action that moved toward max value state 
         instead of a* = max_a Q(s,a)
         - default st val: 0
         - eta: 0.05

medium.csv:
    - train (q-learning): 26.826 s
    - total: 27.058 s

    - used params/notes:
        - if state value after q-learning is still default, just picked random action 
          for policy.
        - reward shaping: subtract distance from goal (assumed to be right-most position) 
                          from reward to encourage progression toward goal. 
        - default st val: -300
        - eta: 0.05
        - train loops: 3

large.csv (submission 56):
    - score: 14.872
    - train: 794.009 s
    - total: 795.574 s

    - used params/notes:
      - if state value after q-learning is still default, just picked random action 
          for policy.
      - st_val_dft: 0
      - num replays: 5 
      - (decaying) eta: 0.25, -0.05 after each (non-initial) replay 
      - used replay buffer: batch_size = 32
      - other attempted approaches: build T,R model from data and do async VI, deep Q NN (Q with NN func. approx. + train & target networks)
      